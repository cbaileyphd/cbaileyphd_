---
title: Variability and Measure Magnitude in Performance Data
author: Chris Bailey
date: '2019-03-09'
slug: variability-and-measure-magnitude-in-performance-data
categories:
  - Athlete Monitoring
  - Data Science
tags:
  - Baseball
  - Data Science
  - R
subtitle: ''
description: ''
image: ''
output:
  blogdown::html_page:
    toc: true
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## Getting started with monitoring data
Initially, we shoud be primarily concerned with reliability. Like me, you probably pick assessments that have shown to be reliable and valid previously in research. But, you should also continuously evaluate your own reliability. If you are completing your own analysis and producing your own visualizations in R, you can build this into your code somewhat easily. Next we might evaluate our data for normality if we are going to perform any statistical analyses that depend on normality. 

One place that is quite often missed in our initial data screening and analysis is something that heavily influences reliability and validity. It is probably more of an issue than many of us realize because it is so rarely evaluated. I am talking about heteroscedasticity (sometimes spelled with a k). Heteroscedasticity essentially means that the specific variable or key performance indicator (KPI) may vary unequally depending on the measure size. A realy high value may have more variation or the opposite could be true with low values. If we are not testing for this, we are essentially assuming homosecdasticity (equal variance regardless of measure magnitude). Since we deal with human performance data, we may often see extreme values (very high or very low) values on a regular basis. But, if are data are heteroscedastic, maybe we shouldn't be too confident in the validity and reliability of that data.  This post is going to be dealing with and detecting this issue. This can be tested for statistically fairly quickly with some R packages (for example, lmtest or gvlma) and probably with other statistical software as well, but I will show how to see it visually as well. Visualizing it likely helps us understand it further, even if it takes a little longer.

## The data
We'll start by reading in the data and calling on some specific packages that we will use. This data set is from 35 NCAA DIII baseball players who performed 2 maximal effort countermovement jump trials. This specific testing data was selected because it is the first of the year. This was the first time jumping off a force plate for probably all of the new players. If there was going to be an issue with reliability, one should expect it to be early on, with reliability increasing somewhat as athletes become more familiar with jump testing. This seems to be particularly true for the new athletes who aren't used to jumping without an armswing.

The jump analysis has already been performed and a table was created to give us the mean of the trials for each variable as well as the individual trials. Names and body mass have been removed. The other variables include jump height (JH), peak power (PP), rate of force development (RFD), peak propulsive force (PF), and peak landing force (PLF). They are being rounded to 2 decimal places.
```{r}
library(data.table)
library(ggplot2)
library(ggthemes)
library(dplyr)
p <- fread("~/Desktop/BaseballCMJData.csv")
p<-select(p, -Athlete, -Mass)
p<-round(p, 2)
```

As discussed earlier, we should start by evaluating reliability. Here, I'll look at relative reliability with ICCs. Unfortunately
```{r}
library(ICC)
ICCdf<-fread("~/Desktop/BaseballCMJData1.csv")
ICCdf$Athlete<- as.factor(ICCdf$Athlete)
JH<-ICCest(Athlete, JH, data = ICCdf, alpha = 0.05, CI.type = c("THD", "Smith"))
PP<-ICCest(Athlete, PP, data = ICCdf, alpha = 0.05, CI.type = c("THD", "Smith"))
RFD<-ICCest(Athlete, RFD, data = ICCdf, alpha = 0.05, CI.type = c("THD", "Smith"))
PF<-ICCest(Athlete, PF, data = ICCdf, alpha = 0.05, CI.type = c("THD", "Smith"))
PLF<-ICCest(Athlete, PLF, data = ICCdf, alpha = 0.05, CI.type = c("THD", "Smith"))
ICCresults<-data.frame(JH=JH$ICC, PP=PP$ICC, RFD=RFD$ICC, PF=PF$ICC, PLF=PLF$ICC) ##Note that I am specifically only calling the ICC part of the ICCest values produced. As a result, I will not see the 95% confidcence intervals of those values. You should look at those, I am only doing that here because it takes up less screen space. 
ICCresults<-ICCresults %>%
  mutate_if(is.numeric, ~round(., 3))
knitr::kable(ICCresults)
```

Here we see that PLF is the least reliable variable (ICC = 0.633, [0.387 - 0.796]), but all the rest might be considered acceptable >0.7. You don't see the 95%CI's in the table because I excluded them to make it more readable. If you call the ICCest result of each variable, you would get the full information.

Now we can look at absolute reliability measures, in this case, coefficients of variation (CV). The CV is just the ratio of the standard deviation to the mean. It is multiplied by 100 to be represented as a percent. This can also be done with the sjstats package cv function as done below.
```{r}
jhCV<-sjstats::cv(p$JH)*100
ppCV<-sjstats::cv(p$PP)*100
rfdCV<-sjstats::cv(p$RFD)*100
pfCV<-sjstats::cv(p$PF)*100
plfCV<-sjstats::cv(p$PLF)*100

CV<-data.frame(JH = jhCV, PP = ppCV, RFD = rfdCV, PF = pfCV, PLF = plfCV)
CV<-round(CV,2)
knitr::kable(CV)
```

We would like the CVs to be lower than 15%, so we obviously have some violations here. But keep in mind this is the first day of a weekly monitoring protocol, so these values may decrease and become acceptable as time goes on. I would wait a few weeks before making a decision about throwing variables out. 

## Heteroscedasticity
First, lets add in the intraindividual differences between trials for each variable. We will use these to see if those who produce very large or very small values have the same chance to vary as those who do not.
```{r}
p$JHdiff<-abs(p$JH1-p$JH2)
p$PPdiff<-abs(p$PP1-p$PP2)
p$RFDdiff<-abs(p$RFD1-p$RFD2)
p$PFdiff<-abs(p$PF1-p$PF2)
p$PLFdiff<-abs(p$PLF1-p$PLF2)
head(p)
```

### Start visually
Let's visually evaluate for the presence of heteroscedasticity by plotting the individual differences against the means. This is essentially creating a linear model where the measure variance (i.e. JHdiff) is being predicted by the measure magnitude (mean of the variable trials). If the data are homoscedastic (equal chance of variablity no matter what the measure magnitude is), then the data that aren't clustered around the linear model should not have any trend. If the data are heteroscedastic, then a trend may be visible (more variance at either high or low values). If you were to draw an outline of all the data points and the shape is a rectangle, then the data are likely homoscedastic. If the shape resembles more of a triangle/funnel with the data close to the line at one side and more spread out along the other, then the data are likely heteroscedastic. Let's take a look.
```{r}
ggplot(p, aes(JH,JHdiff))+geom_point()+ geom_smooth(method='lm') +
  theme_minimal() +
  ggtitle("Jump Height (cm)") +
  ylab("Residuals") +
  xlab("JH means")
```

Using geom_encircle from the ggalt package, I may be able to better display what I mean by outlining the data and its shape. This step likely is not necessaary once the point is understood. 

```{r}
library(ggalt)
ggplot(p, aes(JH,JHdiff))+geom_point()+ geom_smooth(method='lm') +
  theme_minimal() +
  ggtitle("Jump Height (cm)") +
  ylab("Residuals") +
  xlab("JH means") + 
  geom_encircle()
```

In this way we can see how the shape is more broad toward the higher jump height values. This is also a good opportunity to see how a couple of data points (in the top right) might be able skew our interpretation. But there are still quite a few others that deviate from the model below that as well. This may visually indicate that jump height is heteroscedastic, but statistical evidence is necessary to confirm this.

Let's do the same thing with another variable (peak power).
```{r}
ggplot(p, aes(PP,PPdiff))+geom_point()+ geom_smooth(method='lm') +
  theme_minimal() +
  ggtitle("Peak Power (watts)") +
  ylab("Residuals") +
  xlab("PP means") + 
  geom_encircle()
```

Here we can see that the shape is more rectangular. There are a few points deviating from the model, but not necessarily trended toward either end. This should visually indicate that peak power in this sample is homoscedastic.


Now let's look at all the data plotted.

```{r, include=FALSE}
JH<-ggplot(p, aes(JH,JHdiff))+geom_point()+ geom_smooth(method='lm') + 
  theme_minimal() +
  ggtitle("Jump Height (cm)") +
  ylab("Residuals") +
  xlab("JH means") 


PP<-ggplot(p, aes(PP,PPdiff))+geom_point()+ geom_smooth(method='lm') + 
  theme_minimal() +
  ggtitle("Peak Power (watts)") +
  ylab("Residuals") +
  xlab("PP means")

RFD<-ggplot(p, aes(RFD,RFDdiff))+geom_point()+ geom_smooth(method='lm') + 
  theme_minimal() +
  ggtitle("Rate of Force Development (N/s)") +
  ylab("Residuals") +
  xlab("RFD means")

PF<-ggplot(p, aes(PF,PFdiff))+geom_point()+ geom_smooth(method='lm') + 
  theme_minimal() +
  ggtitle("Peak Force (N)") +
  ylab("Residuals") +
  xlab("PF means")

PLF<-ggplot(p, aes(PLF,PLFdiff))+geom_point()+ geom_smooth(method='lm') + 
  theme_minimal() +
  ggtitle("Peak Landing Force (N)") +
  ylab("Residuals") +
  xlab("PLF means") 
```

```{r}
gridExtra::grid.arrange(JH, PP, RFD, PF, PLF)
```

Looking at the plots, we can definitely see that there are some data points that deviate away from the models. Specifically looking at peak landing force, there may be a noticeable trend in that those that land with the highest forces also have the most measurement variaability.

## Add in some statistics
Now let's add some statistical evidince with a Breusch-Pagan test from the lmtest package. We'll start with jump height. We first need to create a linear model predicting the measure variance by the measure magnitude, similar to the plots above.

```{r}
library(lmtest)
##Create the model
JHm<-lm(p$JHdiff~p$JH)
##run the Breusch-Pagan test on the model
bptest(JHm)
```
That statistically significant value of 0.002898 (<0.01) indicates that we do have heteroscedasticity in JH. We are rejecting the null hypothesis that heteroscedasticity is not present. This is a decent sample size with 35 baseball players, but it looks like we have a couple of data points that may be skewing the results as noted earlier. Give it a few weeks and this may go away. That being said, this could be an actual issue given the other points deviating from the model. A larger sample would help to either confirm or reject this notion.

It's also worth noting that there is an easier way to plot the linear model than was done above. It's not quite as pretty, but it's faster if you've already created the model. Once the model has been created plot(modelname) will produce several plots of the model. The first plot shows the residual values plotted against the fitted model pretty much identically to the ones above. 

```{r}
par(mfrow=c(2,2))
plot(JHm)
```

We can also check to see how great our model is (predicting the amount of variablity in JH by the measure magnitude).
```{r}
summary(JHm)
```
Our model fit is statistically significant (p = 0.008921), but it is also important to evaluate its practical significance. It only has an adjusted-R squared of 0.165, so we can predict 16.5% of the variance's variance by the measure magnitude alone.

Let's look at the rest of the BPtest results. Here's peak power.
```{r}
library(lmtest)
PPm<-lm(p$PPdiff~p$PP)
bptest(PPm)
```

RFD
```{r}
library(lmtest)
RFDm<-lm(p$RFDdiff~p$RFD)
bptest(RFDm)
```
We are close with RFD, or we may have evidence of heteroscedasticity depending on the critical value you chose (p<0.01 or p<0.05)

PF
```{r}
library(lmtest)
PFm<-lm(p$PFdiff~p$PF)
bptest(PFm)
```

and finally peak landing force (PLF)
```{r}
library(lmtest)
PLFm<-lm(p$PLFdiff~p$PLF)
bptest(PLFm)
```

It is interesting that we do contain heteroscedasticity in PLF. Given landing force's generally unreliable nature, I would have assumed the chance to vary was the same (very high) no matter the magnitude. This may also be influenced by the usage of an instantaneous measure as opposed to something like impulse or RFD. Let's check to see how great of a model this one is.

```{r}
summary(PLFm)
```

Again, it is statistically significant, but does not have a huge R squared value.


## Practical Application
- We should evaluate performance data to determine if it is reliable and determine if our data vary in a predictable way. 
- We may have evidence that elite performers or poor performers may have a better chance at producing errors.
- If elite athletes (who produce data towards the extremes on normal data distributions) have a greater chance to produce errors, we may be making decisions based on bad data.
    - Evaluation of data for heteroscedasticity is likely more important in performance monitoring of high level athletes.
- Data transformation may reduce the presence of heteroscedasticity, but may not be practical if the resulting values are not logical.

<center>    
```{r, echo=FALSE}
knitr::include_graphics("https://media.giphy.com/media/12jnTh8Dp0cFJS/200.gif")
```
<font size="2">giphy.com</font></center>
<br>
 <center>[follow me on Twitter](https://twitter.com/CBaileyPhD)</center>
<center> [personal website](http://cbaileyphd.com/)</center>
<br><br>
